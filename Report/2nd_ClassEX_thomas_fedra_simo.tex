% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={2nd\_ClassEX\_thomas\_fedra\_simo},
  pdfauthor={Fedra Ippolito},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\title{2nd\_ClassEX\_thomas\_fedra\_simo}
\author{Fedra Ippolito}
\date{30/1/2021}

\begin{document}
\maketitle

\hypertarget{web-scraping}{%
\section{WEB SCRAPING}\label{web-scraping}}

The first part of the script is about web scraping. We have assigned the
url of the article to an object called url, and then we have checked the
robots.txt file. It said to us that we are not allowed to scrape the
admin section of the web site, for the rest there were not problems.
After this quick check we have made a polite request with the funciron
getURL by the package RCurl:: to identify ourself to the server. In
conclusion of this fist phase we have downloaded the html file to work
in local to get the 105 links in the page with function getHTMLlinks by
the package XML::.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(rvest)}
\KeywordTok{library}\NormalTok{(stringr)}
\CommentTok{#dir.create("File download")}
\CommentTok{#dir.create("Scripts")}
\CommentTok{#dir.create("Report")}

\NormalTok{url <-}\StringTok{ "http://www.beppegrillo.it/un-mare-di-plastica-ci-sommergera/"}
\CommentTok{#browseURL(url)}

\CommentTok{#robots.txt check}
\CommentTok{#browseURL("http://www.beppegrillo.it/robots.txt")}

\CommentTok{#file downloading}
\NormalTok{page <-}\StringTok{ }\NormalTok{RCurl}\OperatorTok{::}\KeywordTok{getURL}\NormalTok{(url, }
               \DataTypeTok{useragent =} \KeywordTok{str_c}\NormalTok{(R.version}\OperatorTok{$}\NormalTok{platform,}
\NormalTok{                   R.version}\OperatorTok{$}\NormalTok{version.string,}\DataTypeTok{sep =} \StringTok{", "}\NormalTok{),}
                  \DataTypeTok{httpheader =} \KeywordTok{c}\NormalTok{(}\DataTypeTok{From =} \StringTok{"xxxx@xxxx.xxx"}\NormalTok{)) }\CommentTok{#your e-mail address}

\CommentTok{#Saving the page}
\KeywordTok{download.file}\NormalTok{(url, }\DataTypeTok{destfile =} \StringTok{"plastica.html"}\NormalTok{)}

\CommentTok{#Get links}
\NormalTok{links <-}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{(XML}\OperatorTok{::}\KeywordTok{getHTMLLinks}\NormalTok{(page))}
\end{Highlighting}
\end{Shaded}

\hypertarget{for-loop}{%
\section{FOR LOOP}\label{for-loop}}

Now the game starts to get complex, but we never give up. Now we had to
get all the links to the articles of the year 2016. The amount of them
is 470 articles distributed homogeniuxly in 47 pages: 10 articles per
page. The first step was to get a list of the links of the pages. To do
that we have played with the links. In fact, each link differs from the
others just by the number of the page at the end of the link. Therefore,
we get those links with the str\_c function by the package stringr::.
Easy, but after this point the problems started. We had to make a loop
to make the process of getting the links automated. To do this we hat to
create an empty list of lists with the function of container. With a
combination of the rep(replicate) and vector functions we have created
an empty list (``sut'') containing 47 sublists. In each list of the
``magic 47'' we have created 10 empty spaces in which put the links to
the articles.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{link_to_pages <-}\StringTok{ }\KeywordTok{str_c}\NormalTok{(}\StringTok{"https://www.beppegrillo.it/category/archivio/2016/page/"}\NormalTok{, }
                       \DecValTok{1}\OperatorTok{:}\DecValTok{47}\NormalTok{, }\StringTok{"/"}\NormalTok{)}
\NormalTok{link_to_pages}
\NormalTok{sut <-}\StringTok{  }\KeywordTok{rep}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\KeywordTok{vector}\NormalTok{(}\DataTypeTok{mode =}\StringTok{"list"}\NormalTok{, }\DataTypeTok{length =} \DecValTok{10}\NormalTok{)), }\DecValTok{47}\NormalTok{)}
\NormalTok{sut}
\end{Highlighting}
\end{Shaded}

At this point it was time to create the for loop to get the links. We
spliced it into two parts. The fist part was about the download of the
47 pages to work later in local. Therefore we have used the
download.file function in combination with str\_c function to download
the pages and to get them a progressive numeration. In addition we have
used the Sys.sleep function, set to one second, for politeness reasons.
Came at this point we were ready to get all the links we need with
another for loop. In the end we have unslisted the object and we
obtained a vector with all the 470 links.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \KeywordTok{seq_along}\NormalTok{(link_to_pages)) \{}
  
  \KeywordTok{download.file}\NormalTok{(}\DataTypeTok{url =}\NormalTok{ link_to_pages[i], }
                \DataTypeTok{destfile =}\NormalTok{ here}\OperatorTok{::}\KeywordTok{here}\NormalTok{(}\StringTok{"File download"}\NormalTok{, }\KeywordTok{str_c}\NormalTok{(}\StringTok{"art"}\NormalTok{, i, }\StringTok{".html"}\NormalTok{)))}
  \KeywordTok{Sys.sleep}\NormalTok{(}\DecValTok{1}\NormalTok{)}
  
\NormalTok{\}}

\ControlFlowTok{for}\NormalTok{(x }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{47}\NormalTok{)\{}
\NormalTok{  sut[[x]][}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{] <-}\StringTok{ }\KeywordTok{read_html}\NormalTok{(here}\OperatorTok{::}\KeywordTok{here}\NormalTok{(}\StringTok{"File download"}\NormalTok{,}
                                         \KeywordTok{str_c}\NormalTok{(}\StringTok{"art"}\NormalTok{, x, }\StringTok{".html"}\NormalTok{))) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{html_nodes}\NormalTok{(}\StringTok{".td_module_10 .td-module-title a"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{html_attr}\NormalTok{(}\StringTok{"href"}\NormalTok{)}
\NormalTok{\}}

\NormalTok{art <-}\StringTok{ }\KeywordTok{unlist}\NormalTok{(sut)}
\NormalTok{art}
\end{Highlighting}
\end{Shaded}

To do the last part of the exercise and get the main texts of each
article we took a similar way to the previous one with the creation of
an empty list of lists. A preliminary consideration is that we have
interpreted the wording ``main text'' as the part of the page where the
text of the articles is actually contained. Often in this part of the
web pages there are You Tube players. They will appear in the text as
links contained in the main text of the articles. Moreover, it is
possible that if in some articles there are not text part, they would be
substituted by the YT players which will appear in our list as links.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\KeywordTok{vector}\NormalTok{(}\DataTypeTok{mode =}\StringTok{"list"}\NormalTok{, }\DataTypeTok{length =} \DecValTok{10}\NormalTok{)), }\DecValTok{47}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{(z }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{470}\NormalTok{)\{}
\NormalTok{  out[[z]] <-}\StringTok{ }\KeywordTok{read_html}\NormalTok{(art[z]) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{html_nodes}\NormalTok{(}\StringTok{".rs_preserve+ div"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{html_text}\NormalTok{()}
\NormalTok{\}}

\NormalTok{out}
\end{Highlighting}
\end{Shaded}

\textbf{\emph{NOTE: outputs have been hidden for easier reading. In
fact, often they are long lists of characters or articles, as in the
case just above, that would have made it tedious to scroll through the
report.}}

\hypertarget{what-does-it-means-to-crawl-and-what-is-a-web-spider}{%
\subsection{What does it means to ``crawl'' and what is a web
spider?}\label{what-does-it-means-to-crawl-and-what-is-a-web-spider}}

The verb crawl means to collect web pages, but a web crawler can also
perform data extraction during crawling. Usually web spiders are
programs that automatically browse and download pages by following
hyperlinks in a methodical and automated manner. Web crawler and web
spider are used like synonyms.

\hypertarget{how-is-it-different-from-scraping}{%
\subsection{How is it different from
scraping?}\label{how-is-it-different-from-scraping}}

The activity of web scraping concerns only the extraction of data from a
website, in fact our scraping was built from two steps: parsing and
extracting contents from URLs. This process was manually done, instead
the crawling is an automated method. Moreover we have tryed to authomize
the process of web scraping adding the for loop.

\hypertarget{try-to-build-a-spider-scraper-what-functions-should-you-use}{%
\subsection{Try to build a spider scraper: what functions should you
use?}\label{try-to-build-a-spider-scraper-what-functions-should-you-use}}

Rcrawler(Website = ``\url{https://www.beppegrillo.it/}'', no\_cores = 4,
no\_conn = 4, dataUrlfilter =``./category/archivio/2016/'',
crawlUrlfilter=``./category/archivio/2016/'', ExtractCSSPat =
c(``.td\_module\_10 .td-module-title a'',``.td-excerpt''), PatternsNames
= c(``Title'',``Content''))

dataUrlfilter , crawlUrlfilter \textless- are useful to filter URLs to
be crawled and collected by Regex

\end{document}
